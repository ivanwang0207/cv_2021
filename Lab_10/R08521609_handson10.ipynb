{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"lab12_r08521609.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"dd9D_LNPpUUJ"},"source":["# Convolution Neural Network\n","The goal of this assignment is to get hands-on experience designing and training deep convolutional neural networks using PyTorch. Starting from a baseline architecture we provided, you will design an improved deep net architecture to classify (small) images into 10 construction equipment categories. You will evaluate the performance of your architecture by uploading your predictions to this Kaggle competition and submit your code and report describing your implementation choices to NTU COOL."]},{"cell_type":"markdown","metadata":{"id":"wmCbxoDrADcF"},"source":["# Google Colab setup with Google Drive folder\n","\n","This notebook provides the code you need to set up Google Colab to run and import files from within a Google Drive folder.\n","\n","This will allow you to upload assignment code to your Google Drive and then run the code on Google Colab machines (with free GPUs if needed). \n","\n","You will need to create a folder in your Google Drive to hold your assignments and you will need to open Colaboratory within this folder before running the set up code (check the link above to see how)."]},{"cell_type":"markdown","metadata":{"id":"zWhrmhqVCyGH"},"source":["# Mount Google Drive\n","\n","This will allow the Colab machine to access Google Drive folders by mounting the drive on the machine. You will be asked to copy and paste an authentication code."]},{"cell_type":"code","metadata":{"id":"Wv2oKmF9AJtI","executionInfo":{"status":"ok","timestamp":1622101201341,"user_tz":-480,"elapsed":4,"user":{"displayName":"Bibek gupta","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgbTEbCmlxuaGjCMumUjj9GYctatdlXybKrlHhsug=s64","userId":"11905679151499655395"}}},"source":["#from google.colab import drive\n","#drive.mount('/content/gdrive/')\n","# !gdown --id '1Ln9gskUvFdg3Y27lmPHPelbGRUleSjCs' --output train\n","# !gdown --id '15e6gxX3FRIQ53e_Zsq9iGp8U8kkXvMZm' --output val\n","# !gdown --id '1MELlUwxd24iH1wY7TjgWigHFXhSZh5xd' --output test\n","# Dropbox\n","# !wget https://www.dropbox.com/s/qk9av5laeu817nf/train?dl=0 -O train\n","# !wget https://www.dropbox.com/s/z1gm6w8wtiwx3xs/val?dl=0 -O val\n","# !wget https://www.dropbox.com/s/wkcsa8ah5gtg7uq/test?dl=0 -O test"],"execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4Qs04PPwDOFy"},"source":["# Change directory to allow imports\n","\n","\n","As noted above, you should create a Google Drive folder to hold all your assignment files. You will need to add this code to the top of any python notebook you run to be able to import python files from your drive assignment folder (you should change the file path below to be your own assignment folder)."]},{"cell_type":"markdown","metadata":{"id":"SJOCaUMilRz_"},"source":["# Copy data to local dir"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0gt2tWEVRneM","executionInfo":{"status":"ok","timestamp":1622101326477,"user_tz":-480,"elapsed":554,"user":{"displayName":"Bibek gupta","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgbTEbCmlxuaGjCMumUjj9GYctatdlXybKrlHhsug=s64","userId":"11905679151499655395"}},"outputId":"ff435be0-871c-4ef2-ba75-11450f41a0ce"},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":7,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rJ41kt2qRw2g","executionInfo":{"status":"ok","timestamp":1622101329461,"user_tz":-480,"elapsed":595,"user":{"displayName":"Bibek gupta","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgbTEbCmlxuaGjCMumUjj9GYctatdlXybKrlHhsug=s64","userId":"11905679151499655395"}},"outputId":"686162c2-14f5-4ac5-a580-afa4a6515952"},"source":["ls"],"execution_count":8,"outputs":[{"output_type":"stream","text":["\u001b[0m\u001b[01;34mgdrive\u001b[0m/  \u001b[01;34msample_data\u001b[0m/\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"4r7ZhC5HRij4","executionInfo":{"status":"ok","timestamp":1622101361927,"user_tz":-480,"elapsed":501,"user":{"displayName":"Bibek gupta","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgbTEbCmlxuaGjCMumUjj9GYctatdlXybKrlHhsug=s64","userId":"11905679151499655395"}}},"source":["import os\n","os.chdir(\"/content/gdrive/MyDrive/Colab Notebooks\")"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kqbWt4zhxK2Q","executionInfo":{"status":"ok","timestamp":1622101366868,"user_tz":-480,"elapsed":1755,"user":{"displayName":"Bibek gupta","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgbTEbCmlxuaGjCMumUjj9GYctatdlXybKrlHhsug=s64","userId":"11905679151499655395"}},"outputId":"f820a014-2e4a-4d83-85f4-294856d55711"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":11,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"90MxG_eRla0W","executionInfo":{"status":"ok","timestamp":1622101369700,"user_tz":-480,"elapsed":495,"user":{"displayName":"Bibek gupta","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgbTEbCmlxuaGjCMumUjj9GYctatdlXybKrlHhsug=s64","userId":"11905679151499655395"}}},"source":["#!mkdir /data\n","#!cp train /data/\n","#!cp val /data/\n","#!cp test /data/"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"id":"XvFEFItpl98p","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1622101371924,"user_tz":-480,"elapsed":4,"user":{"displayName":"Bibek gupta","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgbTEbCmlxuaGjCMumUjj9GYctatdlXybKrlHhsug=s64","userId":"11905679151499655395"}},"outputId":"674067d6-466c-43c1-fd29-f2ebce8a7e1e"},"source":["ls ./data"],"execution_count":13,"outputs":[{"output_type":"stream","text":["ls: cannot access './data': Transport endpoint is not connected\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"DDU5aVgR9QBx"},"source":["# Set up GPU and PyTorch\n","\n","First, ensure that your notebook on Colaboratory is set up to use GPU. After opening the notebook on Colaboratory, go to Edit>Notebook settings, select Python 3 under \"Runtime type,\" select GPU under \"Hardware accelerator,\" and save.\n","\n","Next, install PyTorch:"]},{"cell_type":"code","metadata":{"id":"kjbQtzKT9Uc2","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1622101388946,"user_tz":-480,"elapsed":3759,"user":{"displayName":"Bibek gupta","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgbTEbCmlxuaGjCMumUjj9GYctatdlXybKrlHhsug=s64","userId":"11905679151499655395"}},"outputId":"71414813-9e07-424c-dfec-385bd8356a53"},"source":["!pip3 install torch torchvision"],"execution_count":14,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.8.1+cu101)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (0.9.1+cu101)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (3.7.4.3)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch) (1.19.5)\n","Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision) (7.1.2)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"u_BekZYY9Vzx"},"source":["Make sure that pytorch is installed and works with GPU:"]},{"cell_type":"code","metadata":{"id":"8TXSJWQa9efx","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1622101405906,"user_tz":-480,"elapsed":14737,"user":{"displayName":"Bibek gupta","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgbTEbCmlxuaGjCMumUjj9GYctatdlXybKrlHhsug=s64","userId":"11905679151499655395"}},"outputId":"761afab1-05c9-4c51-96c8-022604f8974e"},"source":["import torch\n","a = torch.Tensor([1]).cuda()\n","print(a)\n","!nvidia-smi"],"execution_count":15,"outputs":[{"output_type":"stream","text":["tensor([1.], device='cuda:0')\n","Thu May 27 07:43:15 2021       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 465.19.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   35C    P0    33W / 250W |    897MiB / 16280MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","+-----------------------------------------------------------------------------+\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"qChgLJERsvZP"},"source":["# Part 1"]},{"cell_type":"code","metadata":{"id":"IlyCnvf6WzjR","executionInfo":{"status":"ok","timestamp":1622101415174,"user_tz":-480,"elapsed":879,"user":{"displayName":"Bibek gupta","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgbTEbCmlxuaGjCMumUjj9GYctatdlXybKrlHhsug=s64","userId":"11905679151499655395"}}},"source":["\"\"\"Headers\"\"\"\n","\n","from __future__ import print_function\n","from PIL import Image\n","import os\n","import os.path\n","import numpy as np\n","import sys\n","import cv2\n","if sys.version_info[0] == 2:\n","    import cPickle as pickle\n","else:\n","    import pickle\n","\n","import torch.utils.data as data\n","from torchvision.datasets.utils import download_url, check_integrity\n","\n","import csv\n","%matplotlib inline\n","import matplotlib\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import os.path\n","import sys\n","import torch\n","import torch.utils.data\n","import torchvision\n","import torchvision.transforms as transforms\n","\n","from torch.autograd import Variable\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from tqdm.auto import tqdm\n","np.random.seed(111)\n","torch.cuda.manual_seed_all(111)\n","torch.manual_seed(111)\n","\n","import IPython.display\n","from google.colab.patches import cv2_imshow"],"execution_count":16,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"137GhZMrcTuj"},"source":["\n","\n","## **Just execute the cell below. This is the dataloader. DO NOT CHANGE ANYTHING IN HERE!**\n"]},{"cell_type":"code","metadata":{"id":"URUH4fzzWqKr","executionInfo":{"status":"ok","timestamp":1622101422800,"user_tz":-480,"elapsed":1123,"user":{"displayName":"Bibek gupta","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgbTEbCmlxuaGjCMumUjj9GYctatdlXybKrlHhsug=s64","userId":"11905679151499655395"}}},"source":["\"\"\"\"\"\"\n","class ACID100_CIE5141(data.Dataset):\n","    \"\"\"`ACID <https://www.acidb.ca/dataset>`_ Dataset.\n","    Randomly pick 1000 train, val images and 200 test images for each class that only contains one equipment in the image.\n","    code adapted from CIFAR 100\n","\n","    Args:\n","        root (string): Root directory of dataset where directory\n","            ``cifar-10-batches-py`` exists or will be saved to if download is set to True.\n","        train (bool, optional): If True, creates dataset from training set, otherwise\n","            creates from test set.\n","        transform (callable, optional): A function/transform that  takes in an PIL image\n","            and returns a transformed version. E.g, ``transforms.RandomCrop``\n","        target_transform (callable, optional): A function/transform that takes in the\n","            target and transforms it.\n","        download (bool, optional): If true, downloads the dataset from the internet and\n","            puts it in root directory. If dataset is already downloaded, it is not\n","            downloaded again.\n","\n","    \"\"\"\n","\n","    def __init__(self, root, fold=\"train\",\n","                 transform=None, target_transform=None,\n","                 download=False):\n","        \n","        fold = fold.lower()\n","\n","        self.train = False\n","        self.test = False\n","        self.val = False\n","\n","        if fold == \"train\":\n","            self.train = True\n","        elif fold == \"test\":\n","            self.test = True\n","        elif fold == \"val\":\n","            self.val = True\n","        else:\n","            raise RuntimeError(\"Not train-val-test\")\n","\n","\n","        self.root = os.path.expanduser(root)\n","        self.transform = transform\n","        self.target_transform = target_transform\n","\n","        # fpath = os.path.join(root, self.filename)\n","        # if not self._check_integrity():\n","        #     raise RuntimeError('Dataset not found or corrupted.' +\n","        #                        ' Download it and extract the file again.')\n","\n","        # now load the picked numpy arrays\n","        if self.train:\n","            self.train_data = []\n","            self.train_labels = []\n","            file = os.path.join(self.root, 'train')\n","            print(file)\n","            fo = open(file, 'rb')\n","            if sys.version_info[0] == 2:\n","                entry = pickle.load(fo)\n","            else:\n","                entry = pickle.load(fo, encoding='latin1')\n","            \n","            #print(entry['images'][0].shape)\n","            #image = cv2.cvtColor(entry['images'][0], cv2.COLOR_BGR2RGB)\n","            #print(image)\n","            #cv2_imshow(image)\n","            #plt.imshow(image)\n","            #plt.show()\n","\n","            self.train_data.append(entry['images'])\n","            # make labels from 0-9\n","            self.train_labels = [x-1 for x in entry['labels']]\n","            fo.close()\n","\n","            self.train_data = np.concatenate(self.train_data)\n","            #print(self.train_data.shape)\n","            self.train_data = self.train_data.reshape((1000, 3, 256, 256))\n","            self.train_data = self.train_data.transpose((0, 2, 3, 1))  # convert to HWC\n","            #print(self.train_data.shape)\n","            \n","            p = np.arange(0,1000,1)\n","            mask_train = np.ones((1000,), dtype=bool)\n","            mask_train[p] = True\n","\n","            copy_all_data = np.array(self.train_data)\n","            self.train_data = np.array(copy_all_data[mask_train])\n","            \n","            copy_all_labels = np.array(self.train_labels)\n","            self.train_labels = np.array(copy_all_labels[mask_train])\n","\n","        elif self.val:\n","            self.val_data = []\n","            self.val_labels = []\n","            file = os.path.join(self.root, 'val')\n","            fo = open(file, 'rb')\n","            if sys.version_info[0] == 2:\n","                entry = pickle.load(fo)\n","            else:\n","                entry = pickle.load(fo, encoding='latin1')\n","            \n","            self.val_data.append(entry['images'])\n","            # make labels from 0-9\n","            self.val_labels = [x-1 for x in entry['labels']]\n","            fo.close()\n","\n","            self.val_data = np.concatenate(self.val_data)\n","            self.val_data = self.val_data.reshape((1000, 3, 256, 256))\n","            self.val_data = self.val_data.transpose((0, 2, 3, 1))  # convert to HWC\n","            \n","            p = np.arange(0,1000,1)\n","            mask_val = np.ones((1000,), dtype=bool)\n","            mask_val[p] = True\n","\n","            copy_all_data = np.array(self.val_data)\n","            self.val_data = np.array(copy_all_data[mask_val])\n","            \n","            copy_all_labels = np.array(self.val_labels)\n","            self.val_labels = np.array(copy_all_labels[mask_val])\n","\n","        elif self.test:\n","            # f = self.test_list[0][0]\n","            file = os.path.join(self.root, 'test')\n","            fo = open(file, 'rb')\n","            if sys.version_info[0] == 2:\n","                entry = pickle.load(fo)\n","            else:\n","                entry = pickle.load(fo, encoding='latin1')\n","            self.test_data = entry['images']\n","            # make labels from 0-9\n","            self.test_labels = [x-1 for x in entry['labels']]\n","\n","            fo.close()\n","            self.test_data = self.test_data.reshape((200, 3, 256, 256))\n","            self.test_data = self.test_data.transpose((0, 2, 3, 1))  # convert to HWC\n","\n","    def __getitem__(self, index):\n","        \"\"\"\n","        Args:\n","            index (int): Index\n","\n","        Returns:\n","            tuple: (image, target) where target is index of the target class.\n","        \"\"\"\n","        if self.train:\n","            img, target = self.train_data[index], self.train_labels[index]\n","        elif self.test:\n","            img, target = self.test_data[index], self.test_labels[index]\n","        elif self.val:\n","            img, target = self.val_data[index], self.val_labels[index]\n","\n","        # doing this so that it is consistent with all other datasets\n","        # to return a PIL Image\n","        img = Image.fromarray(cv2.cvtColor(img,cv2.COLOR_BGR2RGB))\n","\n","        if self.transform is not None:\n","            img = self.transform(img)\n","\n","        if self.target_transform is not None:\n","            target = self.target_transform(target)\n","\n","        return img, target\n","\n","    def __len__(self):\n","        if self.train:\n","            return len(self.train_data)\n","        elif self.test:\n","            return len(self.test_data)\n","        elif self.val:\n","            return len(self.val_data)\n","\n","    def _check_integrity(self):\n","        root = self.root\n","        for fentry in (self.train_list + self.test_list):\n","            filename, md5 = fentry[0], fentry[1]\n","            fpath = os.path.join(root, self.base_folder, filename)\n","            if not check_integrity(fpath, md5):\n","                return False\n","        return True\n","\n","    def __repr__(self):\n","        fmt_str = 'Dataset ' + self.__class__.__name__ + '\\n'\n","        fmt_str += '    Number of datapoints: {}\\n'.format(self.__len__())\n","        tmp = 'train' if self.train is True else 'test'\n","        fmt_str += '    Split: {}\\n'.format(tmp)\n","        fmt_str += '    Root Location: {}\\n'.format(self.root)\n","        tmp = '    Transforms (if any): '\n","        fmt_str += '{0}{1}\\n'.format(tmp, self.transform.__repr__().replace('\\n', '\\n' + ' ' * len(tmp)))\n","        tmp = '    Target Transforms (if any): '\n","        fmt_str += '{0}{1}'.format(tmp, self.target_transform.__repr__().replace('\\n', '\\n' + ' ' * len(tmp)))\n","        return fmt_str"],"execution_count":17,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JpFMv7HtcII4"},"source":["This file has been adapted from the easy-to-use tutorial released by PyTorch:\n","http://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html\n","\n","Training an image classifier\n","----------------------------\n","\n","We will do the following steps in order:\n","\n","1. Load the CIFAR100_CS543 training, validation and test datasets using\n","   torchvision. Use torchvision.transforms to apply transforms on the\n","   dataset.\n","2. Define a Convolution Neural Network - BaseNet\n","3. Define a loss function and optimizer\n","4. Train the network on training data and check performance on val set.\n","   Plot train loss and validation accuracies.\n","5. Try the network on test data and create .csv file for submission to kaggle"]},{"cell_type":"code","metadata":{"id":"Ld6juH34dWWq","executionInfo":{"status":"ok","timestamp":1622101431634,"user_tz":-480,"elapsed":470,"user":{"displayName":"Bibek gupta","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgbTEbCmlxuaGjCMumUjj9GYctatdlXybKrlHhsug=s64","userId":"11905679151499655395"}}},"source":["# <<TODO#5>> Based on the val set performance, decide how many\n","# epochs are apt for your model.\n","# ---------\n","EPOCHS = 200\n","# ---------\n","\n","IS_GPU = True\n","TEST_BS = 256\n","TOTAL_CLASSES = 10\n","TRAIN_BS = 16\n","PATH_TO_ACID100_CIE5141 = \"./data/\"\n","\n","# Transform:\n","# Stats needed to normalize images\n","input_size = 128\n","means = [0.485, 0.456, 0.406]\n","std_devs = [0.229, 0.224, 0.225]\n","\n","# Other transforms parameters \n","rotation = 30\n","p=0.5"],"execution_count":18,"outputs":[]},{"cell_type":"code","metadata":{"id":"0ENlTTMi-qFD","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1622101437706,"user_tz":-480,"elapsed":6,"user":{"displayName":"Bibek gupta","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgbTEbCmlxuaGjCMumUjj9GYctatdlXybKrlHhsug=s64","userId":"11905679151499655395"}},"outputId":"78076cf2-10f2-44fb-f97c-eba800b408e1"},"source":["ls /data/"],"execution_count":20,"outputs":[{"output_type":"stream","text":["ls: cannot access '/data/': No such file or directory\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"d57CSAj1dfix","executionInfo":{"status":"ok","timestamp":1622101440492,"user_tz":-480,"elapsed":1,"user":{"displayName":"Bibek gupta","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgbTEbCmlxuaGjCMumUjj9GYctatdlXybKrlHhsug=s64","userId":"11905679151499655395"}}},"source":["def calculate_val_accuracy(valloader, is_gpu):\n","    \"\"\" Util function to calculate val set accuracy,\n","    both overall and per class accuracy\n","    Args:\n","        valloader (torch.utils.data.DataLoader): val set \n","        is_gpu (bool): whether to run on GPU\n","    Returns:\n","        tuple: (overall accuracy, class level accuracy)\n","    \"\"\"    \n","    correct = 0.\n","    total = 0.\n","    predictions = []\n","\n","    class_correct = list(0. for i in range(TOTAL_CLASSES))\n","    class_total = list(0. for i in range(TOTAL_CLASSES))\n","\n","    for data in valloader:\n","        images, labels = data\n","        if is_gpu:\n","            images = images.cuda()\n","            labels = labels.cuda()\n","        outputs = net(Variable(images))\n","        _, predicted = torch.max(outputs.data, 1)\n","        predictions.extend(list(predicted.cpu().numpy()))\n","        total += labels.size(0)\n","        correct += (predicted == labels).sum()\n","\n","        c = (predicted == labels).squeeze()\n","        for i in range(len(labels)):\n","            label = labels[i]\n","            class_correct[label] += c[i]\n","            class_total[label] += 1\n","\n","    class_accuracy = 100 * np.divide(class_correct, class_total)\n","    return 100*correct/total, class_accuracy"],"execution_count":21,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aq2qOUaJeAWJ"},"source":["1.** Loading CIFAR100_CS543**\n","\n","We modify the dataset to create CIFAR100_CS543 dataset which consist of 45000 training images (450 of each class), 5000 validation images (50 of each class) and 10000 test images (100 of each class). The train and val datasets have labels while all the labels in the test set are set to 0.\n"]},{"cell_type":"code","metadata":{"id":"C2UcDZmtdfq3","colab":{"base_uri":"https://localhost:8080/","height":362},"executionInfo":{"status":"error","timestamp":1622101444810,"user_tz":-480,"elapsed":318,"user":{"displayName":"Bibek gupta","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgbTEbCmlxuaGjCMumUjj9GYctatdlXybKrlHhsug=s64","userId":"11905679151499655395"}},"outputId":"ba09301b-910d-4d6c-84a0-9cb04a86d017"},"source":["# The output of torchvision datasets are PILImage images of range [0, 1].\n","# Using transforms.ToTensor(), transform them to Tensors of normalized range\n","# [-1, 1].\n","\n","\n","# <<TODO#1>> Use transforms.Normalize() with the right parameters to \n","# make the data well conditioned (zero mean, std dev=1) for improved training.\n","# <<TODO#2>> Try using transforms.RandomCrop() and/or transforms.RandomHorizontalFlip()\n","# to augment training data.\n","# After your edits, make sure that test_transform should have the same data\n","# normalization parameters as train_transform\n","# You shouldn't have any data augmentation in test_transform (val or test data is never augmented).\n","# ---------------------\n","\n","train_transform = transforms.Compose([\n","    transforms.RandomResizedCrop(input_size),                                 \n","    transforms.RandomHorizontalFlip(),\n","    transforms.RandomRotation(rotation),\n","    transforms.RandomGrayscale(p),\n","    transforms.ToTensor(),\n","    transforms.Normalize(means, std_devs)])\n","\n","test_transform = transforms.Compose([\n","    transforms.Resize(input_size),\n","    transforms.ToTensor(),\n","    transforms.Normalize(means, std_devs)])\n","# ---------------------\n","\n","trainset = ACID100_CIE5141(root=PATH_TO_ACID100_CIE5141, fold=\"train\",\n","                                        download=True, transform=train_transform)\n","trainloader = torch.utils.data.DataLoader(trainset, batch_size=TRAIN_BS,\n","                                          shuffle=True, num_workers=2)\n","print(\"Train set size: \"+str(len(trainset)))\n","\n","valset = ACID100_CIE5141(root=PATH_TO_ACID100_CIE5141, fold=\"val\",\n","                                       download=True, transform=test_transform)\n","valloader = torch.utils.data.DataLoader(valset, batch_size=TEST_BS,\n","                                         shuffle=False, num_workers=2)\n","print(\"Val set size: \"+str(len(valset)))\n","\n","testset = ACID100_CIE5141(root=PATH_TO_ACID100_CIE5141, fold=\"test\",\n","                                       download=True, transform=test_transform)\n","testloader = torch.utils.data.DataLoader(testset, batch_size=TEST_BS,\n","                                         shuffle=False, num_workers=2)\n","print(\"Test set size: \"+str(len(testset)))\n","\n","# The 100 classes for CIFAR100\n","classes =['backhoe_loader', 'excavator', 'dump_truck', 'concrete_mixer_truck', 'mobile_crane', 'dozer', 'compactor', 'wheel_loader', 'grader', 'tower_crane']"],"execution_count":22,"outputs":[{"output_type":"stream","text":["./data/train\n"],"name":"stdout"},{"output_type":"error","ename":"OSError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)","\u001b[0;32m<ipython-input-22-efbeab84b8a4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m trainset = ACID100_CIE5141(root=PATH_TO_ACID100_CIE5141, fold=\"train\",\n\u001b[0;32m---> 30\u001b[0;31m                                         download=True, transform=train_transform)\n\u001b[0m\u001b[1;32m     31\u001b[0m trainloader = torch.utils.data.DataLoader(trainset, batch_size=TRAIN_BS,\n\u001b[1;32m     32\u001b[0m                                           shuffle=True, num_workers=2)\n","\u001b[0;32m<ipython-input-17-2b061610624c>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, fold, transform, target_transform, download)\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m             \u001b[0mfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m                 \u001b[0mentry\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mOSError\u001b[0m: [Errno 107] Transport endpoint is not connected: './data/train'"]}]},{"cell_type":"code","metadata":{"id":"5b_fBznndp4W","executionInfo":{"status":"ok","timestamp":1622101482998,"user_tz":-480,"elapsed":506,"user":{"displayName":"Bibek gupta","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgbTEbCmlxuaGjCMumUjj9GYctatdlXybKrlHhsug=s64","userId":"11905679151499655395"}}},"source":["########################################################################\n","# 2. Define a Convolution Neural Network\n","# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","# We provide a basic network that you should understand, run and\n","# eventually improve\n","# <<TODO>> Add more conv layers\n","# <<TODO>> Add more fully connected (fc) layers\n","# <<TODO>> Add regularization layers like Batchnorm.\n","#          nn.BatchNorm2d after conv layers:\n","#          http://pytorch.org/docs/master/nn.html#batchnorm2d\n","#          nn.BatchNorm1d after fc layers:\n","#          http://pytorch.org/docs/master/nn.html#batchnorm1d\n","# This is a good resource for developing a CNN for classification:\n","# http://cs231n.github.io/convolutional-networks/#layers\n","\n","import math\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","class BaseNet(nn.Module):\n","    def __init__(self, img_rows = input_size):\n","        super(BaseNet, self).__init__()\n","        \n","        # <<TODO#3>> Add more conv layers with increasing \n","        # output channels\n","        # <<TODO#4>> Add normalization layers after conv\n","        # layers (nn.BatchNorm2d)\n","\n","        # Also experiment with kernel size in conv2d layers (say 3\n","        # inspired from VGGNet)\n","        # To keep it simple, keep the same kernel size\n","        # (right now set to 5) in all conv layers.\n","        # Do not have a maxpool layer after every conv layer in your\n","        # deeper network as it leads to too much loss of information.\n","\n","        # self.conv1 = nn.Conv2d(3, 6, 5,1,2)\n","        # self.pool = nn.MaxPool2d(2, 2)\n","        # self.conv2 = nn.Conv2d(6, 16, 5,1,2)\n","\n","        self.out_rows = int(img_rows / 16)\n","        self.cnn = nn.Sequential(\n","            nn.Conv2d(3, 16, 3, 1, 1),\n","            nn.BatchNorm2d(16),\n","            nn.ReLU(),\n","            nn.MaxPool2d(2, 2, 0),\n","\n","            nn.Conv2d(16, 32, 3, 1, 1),\n","            nn.BatchNorm2d(32),\n","            nn.ReLU(),\n","            nn.MaxPool2d(2, 2, 0),\n"," \n","            nn.Conv2d(32, 64, 3, 1, 1),\n","            nn.BatchNorm2d(64),\n","            nn.ReLU(),\n","            nn.MaxPool2d(2, 2, 0),\n","\n","            nn.Conv2d(64, 128, 3, 1, 1),\n","            nn.BatchNorm2d(128),\n","            nn.ReLU(),\n","            nn.MaxPool2d(2, 2, 0),\n","        )\n","\n","        # <<TODO#3>> Add more linear (fc) layers\n","        # <<TODO#4>> Add normalization layers after linear and\n","        # experiment inserting them before or after ReLU (nn.BatchNorm1d)\n","        # More on nn.sequential:\n","        # http://pytorch.org/docs/master/nn.html#torch.nn.Sequential\n","        \n","        self.fc_net = nn.Sequential(\n","            nn.Linear(128 * self.out_rows * self.out_rows, 256),\n","            nn.ReLU(),\n","            #nn.Dropout(0.5),\n","            nn.Linear(256, TOTAL_CLASSES),\n","\n","        )\n","        self._initialize_weights()\n","\n","\n","    def _initialize_weights(self):\n","        for m in self.modules():\n","            if isinstance(m, nn.Conv2d):\n","                nn.init.kaiming_uniform_(m.weight)\n","\n","                if m.bias is not None:\n","                    nn.init.constant_(m.bias, 0)\n","            \n","            elif isinstance(m, nn.BatchNorm2d):\n","                nn.init.constant_(m.weight, 1)\n","                nn.init.constant_(m.bias, 0)\n","\n","            elif isinstance(m, nn.Linear):\n","                nn.init.kaiming_uniform_(m.weight)\n","                nn.init.constant_(m.bias, 0)\n","\n","    def forward(self, x):\n","\n","        # <<TODO#3&#4>> Based on the above edits, you'll have\n","        # to edit the forward pass description here.\n","\n","        #x = self.pool(F.relu(self.conv1(x)))\n","        # Output size = 28//2 x 28//2 = 14 x 14\n","        # 252/2 * 252/2 = 126 * 126\n","\n","        #x = self.pool(F.relu(self.conv2(x)))\n","        # Output size = 10//2 x 10//2 = 5 x 5\n","        # 122/2 * 122/2 = 61 * 61\n","\n","        # See the CS231 link to understand why this is 16*5*5!\n","        # This will help you design your own deeper network\n","        x = self.cnn(x)\n","        x = x.view(x.size()[0], -1)\n","        x = self.fc_net(x)\n","\n","        # No softmax is needed as the loss function in step 3\n","        # takes care of that\n","        \n","        return x\n","\n","\n","# Create an instance of the nn.module class defined above:\n","\n","net = BaseNet()\n","\n","# For training on GPU, we need to transfer net and data onto the GPU\n","# http://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html#training-on-gpu\n","if IS_GPU:\n","    net = net.cuda()\n"],"execution_count":23,"outputs":[]},{"cell_type":"code","metadata":{"id":"zAZjIcLOdp-W","executionInfo":{"status":"ok","timestamp":1622101489125,"user_tz":-480,"elapsed":427,"user":{"displayName":"Bibek gupta","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgbTEbCmlxuaGjCMumUjj9GYctatdlXybKrlHhsug=s64","userId":"11905679151499655395"}}},"source":["########################################################################\n","# 3. Define a Loss function and optimizer\n","# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","# Here we use Cross-Entropy loss and SGD with momentum.\n","# The CrossEntropyLoss criterion already includes softmax within its\n","# implementation. That's why we don't use a softmax in our model\n","# definition.\n","\n","import torch.optim as optim\n","criterion = nn.CrossEntropyLoss()\n","\n","# Tune the learning rate.\n","# See whether the momentum is useful or not\n","optimizer = torch.optim.Adam(net.parameters(), lr=0.001,weight_decay=0.001)\n","\n","plt.ioff()\n","fig = plt.figure()\n","device='cuda'\n","min_acc=0"],"execution_count":24,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ku7eF366dyUP","colab":{"base_uri":"https://localhost:8080/","height":228},"executionInfo":{"status":"error","timestamp":1622101491976,"user_tz":-480,"elapsed":3,"user":{"displayName":"Bibek gupta","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgbTEbCmlxuaGjCMumUjj9GYctatdlXybKrlHhsug=s64","userId":"11905679151499655395"}},"outputId":"465d1626-a949-441d-d3c1-9c03644c9475"},"source":["val_loss_over_epochs = []\n","train_loss_over_epochs = []\n","val_accuracy_over_epochs = []\n","train_accuracy_over_epochs = []\n","for epoch in range(EPOCHS):\n","\n","    # ---------- Training ----------\n","    # Make sure the model is in train mode before training.\n","    net.train()\n","\n","    # These are used to record information in training.\n","    train_loss = []\n","    train_accs = []\n","\n","    # Iterate the training set by batches.\n","    for batch in tqdm(trainloader):\n","\n","        # A batch consists of image data and corresponding labels.\n","        imgs, labels = batch\n","\n","        # Forward the data. (Make sure data and model are on the same device.)\n","        logits = net(imgs.to(device))\n","\n","        # Calculate the cross-entropy loss.\n","        # We don't need to apply softmax before computing cross-entropy as it is done automatically.\n","        loss = criterion(logits, labels.to(device))\n","\n","        # Gradients stored in the parameters in the previous step should be cleared out first.\n","        optimizer.zero_grad()\n","\n","        # Compute the gradients for parameters.\n","        loss.backward()\n","\n","        # Update the parameters with computed gradients.\n","        optimizer.step()\n","\n","        # Compute the accuracy for current batch.\n","        acc = (logits.argmax(dim=-1) == labels.to(device)).float().mean()\n","\n","        # Record the loss and accuracy.\n","        train_loss.append(loss.item())\n","        train_accs.append(acc)\n","\n","    # The average loss and accuracy of the training set is the average of the recorded values.\n","    train_loss = sum(train_loss) / len(train_loss)\n","    train_acc = sum(train_accs) / len(train_accs)\n","\n","    # Print the information.\n","    print(f\"[ Train | {epoch + 1:03d}/{EPOCHS:03d} ] loss = {train_loss:.5f}, acc = {train_acc:.5f}\")\n","\n","    # ---------- Validation ----------\n","    # Make sure the model is in eval mode so that some modules like dropout are disabled and work normally.\n","    net.eval()\n","\n","    # These are used to record information in validation.\n","    valid_loss = []\n","    valid_accs = []\n","\n","    # Iterate the validation set by batches.\n","    for batch in tqdm(valloader):\n","\n","        # A batch consists of image data and corresponding labels.\n","        imgs, labels = batch\n","\n","        # We don't need gradient in validation.\n","        # Using torch.no_grad() accelerates the forward process.\n","        with torch.no_grad():\n","          logits = net(imgs.to(device))\n","\n","        # We can still compute the loss (but not the gradient).\n","        loss = criterion(logits, labels.to(device))\n","\n","        # Compute the accuracy for current batch.\n","        acc = (logits.argmax(dim=-1) == labels.to(device)).float().mean()\n","\n","        # Record the loss and accuracy.\n","        valid_loss.append(loss.item())\n","        valid_accs.append(acc)\n","\n","    # The average loss and accuracy for entire validation set is the average of the recorded values.\n","    valid_loss = sum(valid_loss) / len(valid_loss)\n","    valid_acc = sum(valid_accs) / len(valid_accs)\n","    if valid_acc > min_acc:\n","        # Save model if your model improved\n","        min_acc = valid_acc\n","        torch.save(net.state_dict(),'best')  # Save model to specified path\n","        print('Saving model (epoch = {:4d}, val_acc = {:.4f})'\n","          .format(epoch + 1, min_acc))\n","    val_loss_over_epochs.append(valid_loss)\n","    train_loss_over_epochs.append(train_loss)\n","    val_accuracy_over_epochs.append(valid_acc)\n","    train_accuracy_over_epochs.append(train_acc)\n","    # Print the information.\n","    print(f\"[ Valid | {epoch + 1:03d}/{EPOCHS:03d} ] loss = {valid_loss:.5f}, acc = {valid_acc:.5f}\")\n","\n","# -----------------------------\n","\n","\n","# Plot train loss over epochs and val set accuracy over epochs\n","# Nothing to change here\n","# -------------\n","\n","plt.ylabel('loss')\n","plt.plot(np.arange(EPOCHS), train_loss_over_epochs, 'r-',label='train_loss')\n","plt.plot(np.arange(EPOCHS), val_loss_over_epochs, 'b-',label='val_loss')\n","plt.xticks(np.arange(EPOCHS, dtype=int))\n","plt.show()\n","\n","plt.plot(np.arange(EPOCHS), val_accuracy_over_epochs, 'b-',label='val_acc')\n","plt.plot(np.arange(EPOCHS), train_accuracy_over_epochs, 'r-',label='train_acc')\n","plt.ylabel('accuracy')\n","plt.xlabel('Epochs')\n","plt.xticks(np.arange(EPOCHS, dtype=int))\n","plt.show()\n","plt.savefig(\"plot.png\")\n","plt.close(fig)\n","\n","print('Finished Training')\n","print('Saving model ( val_acc = {:.4f})'\n","          .format( min_acc))\n","# # -------------"],"execution_count":25,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-25-9cb30a857ba0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;31m# Iterate the training set by batches.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;31m# A batch consists of image data and corresponding labels.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'trainloader' is not defined"]}]},{"cell_type":"code","metadata":{"id":"U2XKVHb4LcUB","colab":{"base_uri":"https://localhost:8080/","height":353},"executionInfo":{"status":"error","timestamp":1622101501616,"user_tz":-480,"elapsed":532,"user":{"displayName":"Bibek gupta","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgbTEbCmlxuaGjCMumUjj9GYctatdlXybKrlHhsug=s64","userId":"11905679151499655395"}},"outputId":"9e9aefcf-7c73-45c6-cf33-b8b9d3a34f97"},"source":["del net\n","net = BaseNet().to(device)\n","ckpt = torch.load('best', map_location='cpu')  # Load your best model\n","net.load_state_dict(ckpt)"],"execution_count":26,"outputs":[{"output_type":"error","ename":"OSError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)","\u001b[0;32m<ipython-input-26-c366568ce031>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdel\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBaseNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mckpt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'best'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Load your best model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mckpt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    577\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'encoding'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    580\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m             \u001b[0;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'w'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_open_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mOSError\u001b[0m: [Errno 107] Transport endpoint is not connected: 'best'"]}]},{"cell_type":"code","metadata":{"id":"v1GE8t3mRdy9"},"source":["########################################################################\n","# 5. Try the network on test data, and create .csv file\n","# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","########################################################################\n","\n","# Check out why .eval() is important!\n","# https://discuss.pytorch.org/t/model-train-and-model-eval-vs-model-and-model-eval/5744/2\n","net.eval()\n","\n","total = 0\n","predictions = []\n","for data in testloader:\n","    images, labels = data\n","\n","    # For training on GPU, we need to transfer net and data onto the GPU\n","    # http://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html#training-on-gpu\n","    if IS_GPU:\n","        images = images.cuda()\n","        labels = labels.cuda()\n","    \n","    outputs = net(Variable(images))\n","    _, predicted = torch.max(outputs.data, 1)\n","    predictions.extend(list(predicted.cpu().numpy()))\n","    total += labels.size(0)\n","\n","with open('submission_r08521609.csv', 'w') as csvfile:\n","    wr = csv.writer(csvfile, quoting=csv.QUOTE_ALL)\n","    wr.writerow([\"Id\", \"Prediction1\"])\n","    for l_i, label in enumerate(predictions):\n","        wr.writerow([str(l_i), str(label)])\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Sie9tOErVJ8Q","executionInfo":{"status":"ok","timestamp":1622101507382,"user_tz":-480,"elapsed":474,"user":{"displayName":"Bibek gupta","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgbTEbCmlxuaGjCMumUjj9GYctatdlXybKrlHhsug=s64","userId":"11905679151499655395"}}},"source":[""],"execution_count":26,"outputs":[]}]}